---
title: "FinalProject"
author: "Basanta Chalise, Mahi, Richa, Joshua"
date: "April 28, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(doSNOW)){install.packages("doSNOW")}
if(!require(reshape2)){install.packages("reshape2")}
if(!require(dplyr)){install.packages("dplyr")}
if(!require(tidyr)){install.packages("tidyr")}
```

## Main Code that Converts the data #####

```{r, echo=FALSE}
###### Code to be used for Predicting the unlabelled dataset ######

##### Things to look for the file that is supposed to be realeased on 29th of April ######
#### 1. You need to chnage the file name in while reading the file #####
#### If File has An extra column for ROW Number we don't need to do anything with it ######
#### If File doesn't have extra column simply remove -c(1) while reading the csv file #########

###### Once conversion is donw, your data is saved as FinalWideFrom.csv #########
start_time <- Sys.time()
library(doSNOW)
cl <- makeCluster(4, type="SOCK") # 4 - number of cores
registerDoSNOW(cl) # Register Backend Cores for Parallel Computing
data <- read.csv("letterunlabelled.manual.csv", sep=",", header = T)[,-c(1)]
allRowIndices <- c(1:nrow(data)) # row numbers of inputData, that will be processed in parallel
output_parallel <- foreach (rowNum = allRowIndices, .combine = rbind, .packages =c("reshape2", "dplyr", "tidyr") ) %dopar% {
  t <- data[c(rowNum),]
  t.melt <- melt(t)
  t.melt$X <- rep(rep(0:13, each=4), 56)
  t.melt$Y <- rep(0:13, each = 56*4)
  new.df <- t.melt %>% group_by(X,Y)%>%summarise(I = sum(value)/16)
  new.df$Pixel <- seq.int(nrow(new.df))
  widedata <- spread(new.df[, -c(1,2)],key = Pixel, value = I)
  return (widedata)
}
write.csv(output_parallel, "FinalWideForm.csv")
stopCluster(cl) # undo the parallel processing setup
endtime<- Sys.time()
endtime -start_time


```


```{r, echo=FALSE}

############### Its done here ###########################################################
####### For our case, Random Forest does better classification so we will use random Forest #####
#### We have two option for making prediction on Final Dataset ########
### Option 1: We can train our model using those optimized parameter #######
### Option 2: We can simply import the model that was saved before for prediction #######

##### I have attached a code for Option 2 here ###########
library(randomForest)
####### Lets read the model that was saved before ###########
start_time <- Sys.time()
#### Lets read data here ###########
testdata <- read.csv("FinalWideForm.csv", sep=",")[,-1] ## Remove Colmun 1 which has observation number
### Our model is finalmodel ############
final_model <- readRDS("C:/Users/DELETE ME METTE/Desktop/My Folder/Spring 2019/602/final project/Model/Third meeting/for 30000/Final Folder/final_model.rds")
#########################finalmodel <- readRDS("./final_model.rds")
####### Lets make Prediction with this model
rf.predict <- predict(finalmodel, newdata=testdata)
### lets write our prediction to csv file and we will combine result from 4 of us to upload in d2l#####
write.csv(rf.predict, "Prediction.csv")
endtime<- Sys.time()
endtime -start_time

```